{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY0IUGtZawl3"
   },
   "source": [
    "# RAG graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9YJSCWBawl7",
    "outputId": "08189f53-b4ef-496c-f4fe-5ea7f90e0508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (4.46.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (3.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gioel\\desktop\\rag\\.venv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers faiss-cpu networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWvpD2Czawl-",
    "outputId": "6ddd8417-fe2a-4ad3-8ab4-b0306e8d0dcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gioel\\Desktop\\RAG\\.venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XYisOLEawl-"
   },
   "source": [
    "## Create Graph KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qUzfq45Mawl-"
   },
   "outputs": [],
   "source": [
    "# Initialize a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add Machine Nodes with Attributes\n",
    "G.add_node(\"LaserCutter_1\", node_type=\"Machine\", model=\"LC-200\", manufacturer=\"Brand A\")\n",
    "G.add_node(\"LaserCutter_2\", node_type=\"Machine\", model=\"LC-300\", manufacturer=\"Brand B\")\n",
    "\n",
    "# Add KPI Nodes with Descriptions and Normal Ranges\n",
    "G.add_node(\"WorkingTime\", node_type=\"KPI\", description=\"Time actively working\", unit=\"seconds\", normal_min=6, normal_max=10)\n",
    "G.add_node(\"IdleTime\", node_type=\"KPI\", description=\"Time idle but available\", unit=\"seconds\", normal_min=1, normal_max=4)\n",
    "G.add_node(\"OfflineTime\", node_type=\"KPI\", description=\"Time offline and not available\", unit=\"seconds\", normal_min=0, normal_max=2)\n",
    "\n",
    "# Add Directed Relationships (Edges) Between Machines and KPIs\n",
    "G.add_edge(\"LaserCutter_1\", \"WorkingTime\", relationship=\"measures\")\n",
    "G.add_edge(\"LaserCutter_1\", \"IdleTime\", relationship=\"measures\")\n",
    "G.add_edge(\"LaserCutter_1\", \"OfflineTime\", relationship=\"measures\")\n",
    "\n",
    "G.add_edge(\"LaserCutter_2\", \"WorkingTime\", relationship=\"measures\")\n",
    "G.add_edge(\"LaserCutter_2\", \"IdleTime\", relationship=\"measures\")\n",
    "G.add_edge(\"LaserCutter_2\", \"OfflineTime\", relationship=\"measures\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz0f2cZSawl_"
   },
   "source": [
    "## Create embeddings for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byFVJP8Sawl_",
    "outputId": "ee3f8686-f08a-494b-dedf-50fb4eb0d829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaserCutter_1: Machine type Machine, model LC-200, manufactured by Brand A. Measures KPIs: WorkingTime, IdleTime, OfflineTime.\n",
      "LaserCutter_2: Machine type Machine, model LC-300, manufactured by Brand B. Measures KPIs: WorkingTime, IdleTime, OfflineTime.\n",
      "WorkingTime: KPI WorkingTime, measures Time actively working in seconds. Normal range is 6 to 10. Measured by machines: LaserCutter_1, LaserCutter_2.\n",
      "IdleTime: KPI IdleTime, measures Time idle but available in seconds. Normal range is 1 to 4. Measured by machines: LaserCutter_1, LaserCutter_2.\n",
      "OfflineTime: KPI OfflineTime, measures Time offline and not available in seconds. Normal range is 0 to 2. Measured by machines: LaserCutter_1, LaserCutter_2.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate descriptions automatically\n",
    "def generate_descriptions(graph):\n",
    "    descriptions = {}\n",
    "\n",
    "    for node in graph.nodes(data=True):\n",
    "        node_name, attributes = node\n",
    "        node_type = attributes.get(\"node_type\")\n",
    "\n",
    "        if node_type == \"Machine\":\n",
    "            # Find KPIs measured by this machine\n",
    "            kpis = [target for source, target, data in graph.edges(data=True) if source == node_name and data[\"relationship\"] == \"measures\"]\n",
    "            kpi_list = \", \".join(kpis)\n",
    "\n",
    "            # Generate description using template\n",
    "            descriptions[node_name] = (\n",
    "                f\"Machine type {attributes.get('node_type')}, model {attributes.get('model')}, \"\n",
    "                f\"manufactured by {attributes.get('manufacturer')}. Measures KPIs: {kpi_list}.\"\n",
    "            )\n",
    "\n",
    "        elif node_type == \"KPI\":\n",
    "            # Find machines that measure this KPI\n",
    "            machines = [source for source, target, data in graph.edges(data=True) if target == node_name and data[\"relationship\"] == \"measures\"]\n",
    "            machine_list = \", \".join(machines)\n",
    "\n",
    "            # Generate description using template\n",
    "            descriptions[node_name] = (\n",
    "                f\"KPI {node_name}, measures {attributes.get('description')} in {attributes.get('unit')}. \"\n",
    "                f\"Normal range is {attributes.get('normal_min')} to {attributes.get('normal_max')}. \"\n",
    "                f\"Measured by machines: {machine_list}.\"\n",
    "            )\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "# Generate and print descriptions for all nodes\n",
    "generated_descriptions = generate_descriptions(G)\n",
    "for node, desc in generated_descriptions.items():\n",
    "    print(f\"{node}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6XTVp9cnSCC",
    "outputId": "ecc2ed74-6619-4409-c8df-085ff2ea0734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The total number of LaserCutting machines in the country is .\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load the Sentence Transformer model for embedding-based retrieval\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Assuming `generated_descriptions` contains descriptions for each node\n",
    "node_embeddings = {node: embedder.encode(description) for node, description in generated_descriptions.items()}\n",
    "\n",
    "# Convert node embeddings to a matrix for FAISS\n",
    "embeddings_matrix = np.array(list(node_embeddings.values())).astype(\"float32\")\n",
    "embedding_dim = embeddings_matrix.shape[1]\n",
    "\n",
    "# Initialize FAISS index with the correct dimension\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(embeddings_matrix)\n",
    "\n",
    "# Map node names to their index positions in FAISS\n",
    "node_to_id = {node: idx for idx, node in enumerate(node_embeddings.keys())}\n",
    "id_to_node = {idx: node for node, idx in node_to_id.items()}\n",
    "\n",
    "# Function to retrieve top-k nodes using SentenceTransformer embeddings and FAISS\n",
    "def retrieve_top_k_nodes(query, top_k=3):\n",
    "    query_embedding = embedder.encode(query).reshape(1, -1).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    retrieved_nodes = [\n",
    "        {\n",
    "            \"node\": id_to_node[idx],\n",
    "            \"description\": generated_descriptions[id_to_node[idx]],\n",
    "            \"distance\": distances[0][i]\n",
    "        }\n",
    "        for i, idx in enumerate(indices[0])\n",
    "    ]\n",
    "    return retrieved_nodes\n",
    "\n",
    "# Set up the FLAN-T5 model for text generation\n",
    "model_name = \"google/flan-t5-large\"  # or \"google/flan-t5-xl\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create a text generation pipeline with FLAN-T5\n",
    "generator_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Example query for retrieval\n",
    "query = \"How many LaserCutting machines are there?\"\n",
    "retrieved_nodes = retrieve_top_k_nodes(query)\n",
    "\n",
    "# Combine descriptions into a single context string\n",
    "context = \"\\n\".join([f\"{node['node']}: {node['description']}\" for node in retrieved_nodes])\n",
    "\n",
    "# Format the prompt with the context and query for the FLAN-T5 model\n",
    "prompt = f\"\"\"\n",
    "Please answer the following question based on the provided information. Offer a clear and thorough explanation that directly answers the question and includes any important details or context for a better understanding.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Additional information:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Generate the response\n",
    "response = generator_pipeline(prompt)\n",
    "print(\"Response:\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGKxCxKMjG0W",
    "outputId": "ee842a87-4cfd-4652-d88f-22e601f61de5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WorkingTime: KPI WorkingTime, measures Time actively working in seconds. Normal range is 6 to 10. Measured by machines: LaserCutter_1, LaserCutter_2.\n",
      "IdleTime: KPI IdleTime, measures Time idle but available in seconds. Normal range is 1 to 4. Measured by machines: LaserCutter_1, LaserCutter_2.\n",
      "LaserCutter_2: Machine type Machine, model LC-300, manufactured by Brand B. Measures KPIs: WorkingTime, IdleTime, OfflineTime.\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant set: {'LaserCutter_2', 'LaserCutter_1'}\n",
      "retrieved set: {'WorkingTime', 'IdleTime', 'LaserCutter_2'}\n",
      "Precision@k: 0.33, Recall@k: 0.50\n"
     ]
    }
   ],
   "source": [
    "# Funzione per calcolare precision@k e recall@k\n",
    "def evaluate_retrieval(query, relevant_nodes, top_k=3):\n",
    "    # Recupera i top-k nodi per la query\n",
    "    retrieved_nodes = retrieve_top_k_nodes(query, top_k=top_k)\n",
    "    retrieved_node_names = [node[\"node\"] for node in retrieved_nodes]\n",
    "\n",
    "    # Genera un vettore binario per nodi rilevanti e recuperati\n",
    "    relevant_set = set(relevant_nodes)\n",
    "    retrieved_set = set(retrieved_node_names)\n",
    "    \n",
    "    print(\"relevant set: \"+relevant_set.__str__())\n",
    "    print(\"retrieved set: \"+retrieved_set.__str__())\n",
    "\n",
    "    # Precision@k: quanti dei recuperati sono rilevanti\n",
    "    precision_at_k = len(relevant_set & retrieved_set) / top_k\n",
    "\n",
    "    # Recall@k: quanti dei rilevanti totali sono stati recuperati\n",
    "    recall_at_k = len(relevant_set & retrieved_set) / len(relevant_set)\n",
    "\n",
    "    return precision_at_k, recall_at_k\n",
    "\n",
    "# Esempio di utilizzo\n",
    "query = \"How many LaserCutting machines are there?\"\n",
    "relevant_nodes = [\"LaserCutter_1\", \"LaserCutter_2\"]\n",
    "precision, recall = evaluate_retrieval(query, relevant_nodes)\n",
    "print(f\"Precision@k: {precision:.2f}, Recall@k: {recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Scores: {'BLEU': 0, 'ROUGE': {'rouge-1': {'r': 0.16666666666666666, 'p': 0.1, 'f': 0.12499999531250018}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.1, 'f': 0.12499999531250018}}, 'METEOR': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Funzione per valutare la qualità della generazione\n",
    "def evaluate_generation(generated_response, reference_response):\n",
    "    # Tokenizziamo il testo per meteor_score\n",
    "    reference_tokenized = reference_response.split()\n",
    "    generated_tokenized = generated_response.split()\n",
    "\n",
    "    # BLEU\n",
    "    bleu_score = sentence_bleu([reference_tokenized], generated_tokenized)\n",
    "    \n",
    "    # ROUGE\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(generated_response, reference_response, avg=True)\n",
    "    \n",
    "    # METEOR\n",
    "    meteor = meteor_score([reference_tokenized], generated_tokenized)\n",
    "    \n",
    "    return {\n",
    "        \"BLEU\": bleu_score,\n",
    "        \"ROUGE\": rouge_scores,\n",
    "        \"METEOR\": meteor\n",
    "    }\n",
    "\n",
    "# Esempio di utilizzo\n",
    "reference_response = \"There are 2 laser cutting machines.\"\n",
    "generated_response = response[0][\"generated_text\"]\n",
    "\n",
    "scores = evaluate_generation(generated_response, reference_response)\n",
    "print(\"Evaluation Scores:\", scores)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
